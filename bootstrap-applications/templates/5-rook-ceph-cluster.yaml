apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: rook-ceph-cluster
  namespace: argocd
  annotations:
    argocd.argoproj.io/sync-wave: "5"
  finalizers:
  - resources-finalizer.argocd.argoproj.io
spec:
  project: default
  source:
    repoURL: 'https://charts.rook.io/release'
    targetRevision: v1.10.8
    helm:
      values: |
        toolbox:
          enabled: true
          resources:
            limits:
              cpu: null
              memory: "128Mi"
            requests:
              cpu: "100m"
              memory: "128Mi"
        monitoring:
          enabled: true
          createPrometheusRules: true
        cephClusterSpec:
          cephVersion:
            image: quay.io/ceph/ceph:v17.2.5
            allowUnsupported: false
          dataDirHostPath: /var/lib/rook
          skipUpgradeChecks: false
          continueUpgradeAfterChecksEvenIfNotHealthy: false
          waitTimeoutForHealthyOSDInMinutes: 10
          mon:
            count: 3
            allowMultiplePerNode: false
          mgr:
            count: 2
            allowMultiplePerNode: false
            modules:
              - name: pg_autoscaler
                enabled: true
          dashboard:
            enabled: true
            ssl: false
          crashCollector:
            disable: false
          cleanupPolicy:
            confirmation: ""
            sanitizeDisks:
              method: quick
              dataSource: zero
              iteration: 1
            allowUninstallWithVolumes: false
          resources:
            mgr:
              limits:
                cpu: null
                memory: "1Gi"
              requests:
                cpu: "100m"
                memory: "1Gi"
            mon:
              limits:
                cpu: null
                memory: "2Gi"
              requests:
                cpu: "100m"
                memory: "2Gi"
            osd:
              limits:
                cpu: null
                memory: "3Gi"
              requests:
                cpu: "100m"
                memory: "3Gi"
            prepareosd:
              limits:
                cpu: null
                memory: "400Mi"
              requests:
                cpu: "100m"
                memory: "400Mi"
            mgr-sidecar:
              limits:
                cpu: null
                memory: "100Mi"
              requests:
                cpu: "100m"
                memory: "100Mi"
            crashcollector:
              limits:
                cpu: null
                memory: "60Mi"
              requests:
                cpu: "100m"
                memory: "60Mi"
            logcollector:
              limits:
                cpu: null
                memory: "1Gi"
              requests:
                cpu: "100m"
                memory: "1Gi"
            cleanup:
              limits:
                cpu: null
                memory: "1Gi"
              requests:
                cpu: "500m"
                memory: "1Gi"
          removeOSDsIfOutAndSafeToRemove: false
          priorityClassNames:
            mon: system-node-critical
            osd: system-node-critical
            mgr: system-cluster-critical
          storage: # cluster level storage configuration and selection
            useAllNodes: true
            useAllDevices: true
          disruptionManagement:
            managePodBudgets: true
            osdMaintenanceTimeout: 30
            pgHealthCheckTimeout: 0
            manageMachineDisruptionBudgets: false
            machineDisruptionBudgetNamespace: openshift-machine-api
          healthCheck:
            daemonHealth:
              mon:
                disabled: false
                interval: 45s
              osd:
                disabled: false
                interval: 60s
              status:
                disabled: false
                interval: 60s
            livenessProbe:
              mon:
                disabled: false
              mgr:
                disabled: false
              osd:
                disabled: false
        ingress:
          dashboard:
            annotations:
              kubernetes.io/ingress.class: traefik
              traefik.ingress.kubernetes.io/router.entrypoints: websecure
              traefik.ingress.kubernetes.io/router.tls: "true"
              traefik.ingress.kubernetes.io/router.middlewares: chowned@file
            host:
              name: rook-dashboard.chowned.net
            tls: {}
        cephBlockPools:
          - name: rook-ceph-block
            spec:
              failureDomain: host
              replicated:
                size: 3
            storageClass:
              enabled: true
              name: rook-ceph-block
              isDefault: true
              reclaimPolicy: Retain
              allowVolumeExpansion: true
              mountOptions: []
              parameters:
                imageFormat: "2"
                imageFeatures: layering
                csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
                csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
                csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
                csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
                csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
                csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph
                csi.storage.k8s.io/fstype: ext4
        cephObjectStores: {}
        cephFileSystems: {}
    chart: rook-ceph-cluster
  destination:
    server: 'https://kubernetes.default.svc'
    namespace: rook-ceph
  syncPolicy:
    automated:
      selfHeal: true
      prune: true
    syncOptions:
      - CreateNamespace=true